{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0259765",
   "metadata": {
    "id": "c0259765"
   },
   "source": [
    "# Machine Learning Exercises 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e775045f",
   "metadata": {
    "id": "e775045f"
   },
   "source": [
    "## 1 - Naive Bayes Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9372fb",
   "metadata": {
    "id": "ed9372fb"
   },
   "source": [
    "![](Q1_1.png \"Q1_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5a43e9",
   "metadata": {
    "id": "0a5a43e9"
   },
   "source": [
    "![](Q1_2.png \"Q1_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025971fb",
   "metadata": {},
   "source": [
    "Question 1 Answer:\n",
    "We want P(buys_computer = yes | age = youth, income = medium, student = yes, credit_rating = fair)\n",
    "    and P(buys_computer = no | age = youth, income = medium, student = yes, credit_rating = fair)\n",
    "\n",
    "Our answer is the largest of the two (the argmax class). We will ignore the denominators as they are the same for both calculations.\n",
    "\n",
    "P(yes | youth, medium, styes, fair) => P(yes) * P(youth | yes) * P(medium | yes) * P(styes | yes) * P(fair | yes)\n",
    " => (9/14) * (2/9) * (4/9) * (6/9) * (6/9) = 0.0282\n",
    " \n",
    "P(no | youth, medium, styes, fair) => P(no) * P(youth | no) * P(medium | no) * P(styes | no) * P(fair | no)\n",
    " => (5/14) * (3/5) * (2/5) * (1/5) * (2/5) = 0.00686\n",
    " \n",
    "These numbers could be normalized, but that's unnecessary here. It is clear that this customer will buy a computer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c98d11d",
   "metadata": {
    "id": "7c98d11d"
   },
   "source": [
    "## 2 - Classification Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffb6759",
   "metadata": {
    "id": "1ffb6759"
   },
   "source": [
    "![](Q2.png \"Q2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d38540",
   "metadata": {},
   "source": [
    "Question 2 Answer:\n",
    "1. There are 3 predicted positives which are actually negative. There is 1 predicted negative which is actually positive.\n",
    "2. Accuracy = #correct/#total = (33+72)/(33+72+1+3) = 0.963 = 96.3%\n",
    "3. Precision = #truepos/(#truepos+#falsepos) = (33)/(33+3) = 0.917 = 91.7%\n",
    "   Recall = #truepos/(#truepos + #falseneg) = (33)/(33+1) = 0.971 = 97.1%\n",
    "4. F1 = 2*(precision*recall)/(precision+recall) = 2*(0.917*0.971)/(0.917+0.971) = 0.943\n",
    "5. We obviously have greater attention for infected people rather than uninfected people. Thus, we'd care more about the rate of false negatives, who would think they were Covid free and may go on to infect others, which means we'd prioritize a higher recall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca0ce4f",
   "metadata": {
    "id": "0ca0ce4f"
   },
   "source": [
    "## 3 -  Logistic Regression and Perceptron\n",
    "\n",
    " In this problem you will be applying logistic regression and perceptron to the breastcancer dataset for binary classification:\n",
    "\n",
    " **default of credit card clients**:  This dataset contains information on default payments, demographic factors, credit data, history of payment, and bill statements of credit card clients in Taiwan from April 2005 to September 2005.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbee24d",
   "metadata": {
    "id": "fdbee24d"
   },
   "source": [
    "### Task\n",
    "- Prepare a normalized version of data. Use min-max normalization. \n",
    "- Train two logistic regression models using gradient descent with raw as well as normalized data. \n",
    "- Train two perceptron classifiers with raw as well as normalized data.\n",
    "- Compare training and test results of four models in terms of accuracy. \n",
    "\n",
    "Note:\n",
    "\n",
    "The skeleton code is only a guide. You can change the method definitions where necessary with appropriate comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b7d425c",
   "metadata": {
    "id": "4b7d425c"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f67fca1",
   "metadata": {
    "id": "6f67fca1"
   },
   "outputs": [],
   "source": [
    "def load_data(data_dir):\n",
    "    ''' data: input features\n",
    "        labels: output features\n",
    "    '''\n",
    "    df = pd.read_excel(data_dir)\n",
    "\n",
    "    datanp = df.to_numpy()\n",
    "    labels = datanp[:,-1]\n",
    "    data = np.delete(datanp,-1,1) #don't need the labels in the data\n",
    "    data = np.delete(data,0,1) #don't need the ID in the data either\n",
    "    data = data.astype(float)\n",
    "    return data, labels\n",
    "\n",
    "data, labels = load_data('Credit card Default.xlsx')\n",
    "normdata = np.copy(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31d30cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000.0\n",
      "2.0\n",
      "6.0\n",
      "3.0\n",
      "75.0\n",
      "8.0\n",
      "7.0\n",
      "7.0\n",
      "7.0\n",
      "7.0\n",
      "8.0\n",
      "964511.0\n",
      "983931.0\n",
      "578971.0\n",
      "891586.0\n",
      "927171.0\n",
      "961664.0\n",
      "368199.0\n",
      "344261.0\n",
      "896040.0\n",
      "497000.0\n",
      "332000.0\n",
      "528666.0\n",
      "[[2.000e+04 2.000e+00 2.000e+00 ... 0.000e+00 0.000e+00 0.000e+00]\n",
      " [1.200e+05 2.000e+00 2.000e+00 ... 1.000e+03 0.000e+00 2.000e+03]\n",
      " [9.000e+04 2.000e+00 2.000e+00 ... 1.000e+03 1.000e+03 5.000e+03]\n",
      " ...\n",
      " [3.200e+05 1.000e+00 1.000e+00 ... 7.500e+03 7.500e+03 7.500e+03]\n",
      " [1.000e+05 2.000e+00 2.000e+00 ... 3.000e+03 0.000e+00 0.000e+00]\n",
      " [1.000e+05 2.000e+00 1.000e+00 ... 0.000e+00 3.622e+03 0.000e+00]]\n",
      "[[0.01010101 1.         0.33333333 ... 0.         0.         0.        ]\n",
      " [0.11111111 1.         0.33333333 ... 0.00201207 0.         0.00378311]\n",
      " [0.08080808 1.         0.33333333 ... 0.00201207 0.00301205 0.00945777]\n",
      " ...\n",
      " [0.31313131 0.         0.16666667 ... 0.01509054 0.02259036 0.01418665]\n",
      " [0.09090909 1.         0.33333333 ... 0.00603622 0.         0.        ]\n",
      " [0.09090909 1.         0.16666667 ... 0.         0.01090964 0.        ]]\n",
      "[1 1 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "#Min-max Normalization\n",
    "for i in range(len(normdata[0])): #for each row of data\n",
    "    cmin = np.amin(normdata[:,i])\n",
    "    cmax = np.amax(normdata[:,i])\n",
    "    print(cmax)\n",
    "    normdata[:,i] -= cmin\n",
    "    normdata[:,i] = normdata[:,i]/float(cmax-cmin)\n",
    "    \n",
    "print(data)\n",
    "print(normdata)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb2391d",
   "metadata": {
    "id": "2bb2391d"
   },
   "source": [
    "### 3.1 - Implementation of sigmoid and cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10f982e9",
   "metadata": {
    "id": "10f982e9"
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    ''' return sigmoid'''\n",
    "    sig = 1/(1+math.e**(-1*z)) #small constant to avoid getting sig = 1 for large z's\n",
    "    return sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0b0962e",
   "metadata": {
    "id": "f0b0962e"
   },
   "outputs": [],
   "source": [
    "## Implement the loss function for logistic regression\n",
    "\n",
    "def compute_cost(ip, op, params):\n",
    "    \"\"\"\n",
    "    Cost function in linear regression where the cost is calculated\n",
    "    ip: input variables\n",
    "    op: output variables\n",
    "    params: corresponding parameters\n",
    "    Returns cost\n",
    "    \"\"\"\n",
    "    loss = 0\n",
    "    epsilon = 0.05\n",
    "    #max and mins added to account for cases where colossal dot products overwhelm machine precision\n",
    "    for i in range(len(ip)):\n",
    "        add = -1 * op[i] * math.log(max(sigmoid(np.dot(ip[i], params)), epsilon)) - \\\n",
    "            (1 - op[i]) * math.log(1 - min(sigmoid(np.dot(ip[i], params)),1-epsilon))\n",
    "        loss += add\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced13867",
   "metadata": {
    "id": "ced13867"
   },
   "source": [
    "\n",
    "### 3.2  Implement logistic regression using batch gradient descent and evaluation\n",
    "Algorithm can be given as follows:\n",
    "\n",
    "```for j in 0 -> max_iteration: \n",
    "    for i in 0 -> m: \n",
    "        theta += (alpha / m) * (y[i] - h(x[i])) * x_bar\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb063fa6",
   "metadata": {
    "id": "cb063fa6"
   },
   "outputs": [],
   "source": [
    "def logistic_regression_using_batch_gradient_descent(ip, op, params, alpha, max_iter, batch_size = 1):\n",
    "    \"\"\"\n",
    "    Compute the params for logistic regression using batch gradient descent\n",
    "    ip: input variables\n",
    "    op: output variables\n",
    "    params: corresponding parameters\n",
    "    alpha: learning rate\n",
    "    max_iter: maximum number of iterations\n",
    "    batch_size: size of the batch, 1 for unit batches, len(ip) for one big batch\n",
    "    Returns parameters, cost, params_store\n",
    "    \"\"\" \n",
    "    #initialize iteration, number of samples, cost and parameter array\n",
    "    iteration = 0\n",
    "    num_samples = len(ip)\n",
    "    costs = np.zeros(max_iter)\n",
    "    param_array = np.zeros([max_iter,len(ip[0])])\n",
    "    \n",
    "    #batchify the data into mini-batches, I've done this but why is this comment here when the \n",
    "    #line above this cell says to use batch. It does not say to use minibatch... so I use batch below\n",
    "    x_batches = []\n",
    "    y_batches = []\n",
    "    batch_count = math.ceil(num_samples/batch_size)\n",
    "    for i in range(batch_count):\n",
    "        if i == batch_count-1:\n",
    "            x_batches.append(ip[i*batch_size:,:])\n",
    "            y_batches.append(op[i*batch_size:])\n",
    "        else:\n",
    "            x_batches.append(ip[i*batch_size:(i+1)*batch_size,:])\n",
    "            y_batches.append(op[i*batch_size:(i+1)*batch_size])\n",
    "    \n",
    "    #Compute the cost and store the params for the corresponding cost\n",
    "    while iteration < max_iter:\n",
    "        costs[iteration] = compute_cost(ip,op,params)\n",
    "        param_array[iteration,:] = params\n",
    "        \n",
    "        #assuming 1 batch\n",
    "        sigs = np.zeros(len(ip))\n",
    "        for i in range(len(ip)):\n",
    "            sigs[i] = (sigmoid(np.dot(ip[i], params)))\n",
    "        \n",
    "        params = params - alpha * np.dot(ip.T, (sigs-op))              \n",
    "                \n",
    "        iteration += 1\n",
    "    \n",
    "    return param_array, costs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5e26ce",
   "metadata": {
    "id": "3f5e26ce"
   },
   "source": [
    "### 3.3 - Implementation of perceptron "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68380bd3",
   "metadata": {
    "id": "68380bd3"
   },
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "# constructor \n",
    "    def __init__ (self):\n",
    "        self.w = None\n",
    "        #bias contained within w vector, at end\n",
    "    \n",
    "    def calc(self, x):\n",
    "        yhat = np.dot(self.w,x)\n",
    "        return yhat\n",
    "    \n",
    "    def update(self, x, y, learn_rate):\n",
    "        x = np.append(x,1)\n",
    "        self.w += learn_rate*y*x\n",
    "        \n",
    "    def predict(self, x):\n",
    "        x = np.append(x,1)\n",
    "        yhat = self.calc(x)\n",
    "        if yhat >= 0:\n",
    "            return 1\n",
    "        else:\n",
    "            return -1\n",
    "        \n",
    "    def fit(self, x, y, learn_rate):\n",
    "        no_mistakes = False\n",
    "        i = 0\n",
    "        k = 0\n",
    "        while not no_mistakes:\n",
    "            for j in range(len(x)):\n",
    "                if i == len(x) or k == 5000000: #stop conditions\n",
    "                    no_mistakes = True #either we go through all the data once without error\n",
    "                    break              #or we stop after 5,000,000, because we don't know that it will stop\n",
    "                yhat = self.predict(x[j])\n",
    "                y_temp = 0 #truth\n",
    "                if y[j] == 1:\n",
    "                    y_temp = 1\n",
    "                else:\n",
    "                    y_temp = -1\n",
    "                if yhat*y_temp <= 0: \n",
    "                    self.update(x[j], y_temp, learn_rate)\n",
    "                    i = 0 #we reset if we have had an error\n",
    "                    k += 1\n",
    "                else:\n",
    "                    i += 1\n",
    "                    k += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f84b7b4",
   "metadata": {
    "id": "1f84b7b4"
   },
   "source": [
    "### 3.4 - Apply 80-20 split on data to prepare training and test sets. Report training and test results in terms of accuracy, precision and recall for both logistic regression and perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cae0d71e",
   "metadata": {
    "id": "cae0d71e"
   },
   "outputs": [],
   "source": [
    "# Sample training code cell change according to your variables and structure\n",
    "\n",
    "# Training the model\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "#reserve the test data, do not use them for cross-validation!\n",
    "\n",
    "data, labels = load_data('Credit card Default.xlsx')\n",
    "normdata = np.copy(data)\n",
    "#Min-max Normalization\n",
    "for i in range(len(normdata[0])): #for each row of data\n",
    "    cmin = np.amin(normdata[:,i])\n",
    "    cmax = np.amax(normdata[:,i])\n",
    "    normdata[:,i] -= cmin\n",
    "    normdata[:,i] = normdata[:,i]/float(cmax-cmin)\n",
    "\n",
    "x_train_norm, x_test_norm, y_train_norm, y_test_norm = train_test_split(normdata, labels, test_size = 0.20)\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8582ec4f",
   "metadata": {
    "id": "8582ec4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 780, 216]\n",
      "Accuracy, Precision, and Recall for Perceptron on Raw Data\n",
      "Accuracy: 0.7831325301204819\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccuracy, Precision, and Recall for Perceptron on Raw Data\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccuracy: \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m((conf[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m conf[\u001b[38;5;241m2\u001b[39m])\u001b[38;5;241m/\u001b[39m\u001b[38;5;28msum\u001b[39m(conf)))\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrecision: \u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRecall: \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m((conf[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m/\u001b[39m(conf[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m+\u001b[39mconf[\u001b[38;5;241m3\u001b[39m])))\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "pA = Perceptron()\n",
    "pA.w = np.zeros(len(data[0])+1) #+1 for the bias term at the end\n",
    "pA.fit(x_train, y_train, 0.05)\n",
    "\n",
    "conf = [0, 0, 0, 0] #true positives, false positives, true negatives, false negatives\n",
    "for i in range(len(x_test)):\n",
    "    pred = pA.predict(x_test[i])\n",
    "    if (pred == 1 and y_test[i] == 1):\n",
    "        conf[0] += 1\n",
    "    elif(pred == 1 and y_test[i] == 0):\n",
    "        conf[1] += 1\n",
    "    elif(pred == -1 and y_test[i] == 0):\n",
    "        conf[2] += 1\n",
    "    else:\n",
    "        conf[3] += 1\n",
    "\n",
    "print(conf)\n",
    "print('Accuracy, Precision, and Recall for Perceptron on Raw Data')\n",
    "print('Accuracy: ' + str((conf[0] + conf[2])/sum(conf)))\n",
    "print('Precision: '  + str((conf[0])/(conf[0]+conf[1])))\n",
    "print('Recall: ' + str((conf[0])/(conf[0]+conf[3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "280ff438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[178, 555, 231, 32]\n",
      "996\n",
      "996\n",
      "Accuracy, Precision, and Recall for Perceptron on Normalized Data\n",
      "Accuracy: 0.4106425702811245\n",
      "Precision: 0.24283765347885403\n",
      "Recall: 0.8476190476190476\n"
     ]
    }
   ],
   "source": [
    "pB = Perceptron()\n",
    "pB.w = np.zeros(len(data[0])+1) #+1 for the bias term at the end\n",
    "pB.fit(x_train_norm, y_train_norm, 0.05)\n",
    "\n",
    "conf = [0, 0, 0, 0] #true positives, false positives, true negatives, false negatives\n",
    "for i in range(len(x_test_norm)):\n",
    "    pred = pB.predict(x_test_norm[i])\n",
    "    if (pred == 1 and y_test_norm[i] == 1):\n",
    "        conf[0] += 1\n",
    "    elif(pred == 1 and y_test_norm[i] == 0):\n",
    "        conf[1] += 1\n",
    "    elif(pred == -1 and y_test_norm[i] == 0):\n",
    "        conf[2] += 1\n",
    "    else:\n",
    "        conf[3] += 1\n",
    "print(conf)\n",
    "print(sum(conf))\n",
    "print(len(x_test_norm))\n",
    "print('Accuracy, Precision, and Recall for Perceptron on Normalized Data')\n",
    "print('Accuracy: ' + str((conf[0] + conf[2])/sum(conf)))\n",
    "print('Precision: '  + str((conf[0])/(conf[0]+conf[1])))\n",
    "print('Recall: ' + str((conf[0])/(conf[0]+conf[3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "00a05ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jommc\\AppData\\Local\\Temp\\ipykernel_6288\\3359939133.py:3: RuntimeWarning: overflow encountered in scalar power\n",
      "  sig = 1/(1+math.e**(-1*z)) #small constant to avoid getting sig = 1 for large z's\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy, Precision, and Recall for Logistic Regression on Raw Data\n",
      "Accuracy: 0.7891566265060241\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n"
     ]
    }
   ],
   "source": [
    "params = np.random.rand(len(x_train[0]))\n",
    "lr_params, lr_costs = logistic_regression_using_batch_gradient_descent(x_train, y_train, params, 0.001, 100, 5)\n",
    "\n",
    "correct = 0\n",
    "for i in range(len(x_test)):\n",
    "   \n",
    "    if pred == y_test[i]:\n",
    "        correct += 1\n",
    "\n",
    "conf = [0, 0, 0, 0] #true positives, false positives, true negatives, false negatives\n",
    "for i in range(len(x_test)):\n",
    "    p = sigmoid(np.dot(x_test[i],lr_params[-1]))\n",
    "    if p > 0.5:\n",
    "        pred = 1\n",
    "    else:\n",
    "        pred = 0    \n",
    "    \n",
    "    if (pred == 1 and y_test_norm[i] == 1):\n",
    "        conf[0] += 1\n",
    "    elif(pred == 1 and y_test_norm[i] == 0):\n",
    "        conf[1] += 1\n",
    "    elif(pred == 0 and y_test_norm[i] == 0):\n",
    "        conf[2] += 1\n",
    "    else:\n",
    "        conf[3] += 1\n",
    "\n",
    "print('Accuracy, Precision, and Recall for Logistic Regression on Raw Data')\n",
    "print('Accuracy: ' + str((conf[0] + conf[2])/sum(conf)))\n",
    "print('Precision: '  + str((conf[0])/(conf[0]+conf[1]+0.0001)))\n",
    "print('Recall: ' + str((conf[0])/(conf[0]+conf[3])))\n",
    "\n",
    "#There is some weird stuff going on with the unnormalized data and the sigmoid calculation\n",
    "#I believe (via some testing) that it results in some weird flipflopping between 2 cost values determined by the epsilon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98150581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy, Precision, and Recall for Logistic Regression on Normalized Data\n",
      "Accuracy: 0.8002008032128514\n",
      "Precision: 0.6571428571428571\n",
      "Recall: 0.10952380952380952\n"
     ]
    }
   ],
   "source": [
    "params = np.random.rand(len(x_train_norm[0]))\n",
    "lr_params_norm, lr_costs_norm = logistic_regression_using_batch_gradient_descent(x_train_norm, y_train_norm, params, 0.001, 100, len(x_train_norm))\n",
    "\n",
    "conf = [0, 0, 0, 0] #true positives, false positives, true negatives, false negatives\n",
    "for i in range(len(x_test_norm)):\n",
    "    p = sigmoid(np.dot(x_test_norm[i],lr_params_norm[-1]))\n",
    "    \n",
    "    if p > 0.5:\n",
    "        pred = 1\n",
    "    else:\n",
    "        pred = 0\n",
    "        \n",
    "    if (pred == 1 and y_test_norm[i] == 1):\n",
    "        conf[0] += 1\n",
    "    elif(pred == 1 and y_test_norm[i] == 0):\n",
    "        conf[1] += 1\n",
    "    elif(pred == 0 and y_test_norm[i] == 0):\n",
    "        conf[2] += 1\n",
    "    else:\n",
    "        conf[3] += 1\n",
    "\n",
    "print('Accuracy, Precision, and Recall for Logistic Regression on Normalized Data')\n",
    "print('Accuracy: ' + str((conf[0] + conf[2])/sum(conf)))\n",
    "print('Precision: '  + str((conf[0])/(conf[0]+conf[1])))\n",
    "print('Recall: ' + str((conf[0])/(conf[0]+conf[3])))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "CS6140-Assign_2_Summer22.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
